the hook is going to be the anime girl that just pops up. 
the dataset as well cannot be dissapointing. 

provide motivation and intuition on how dimensionality reduction would be useful. 

scrolly teller probably
animation to fade things in. 
showing the idealized directions of the scroll. 

another option:
you want to perform an analysis on the images


figure out which images belong to which artist

1) 

hutao teaches you about principal component analysis

lets say that we have some images which we want to analyze
we could go through each image individually to figure out what makes them different
if there are only a couple images, it may not be that hard to do this (show a couple of the images)
however, if there are hundreds, or even thousands, (zoom out to draw all of the images), it can become quite time consuming and tedious. 

one way which you can quickly gain insight into your dataset is through reducing its dimensionality and make it visualizable. 
that way you can take a glance at the final visualization and get a feel for high-level patterns. 

if you're reducing the dimensionality of your dataset, it's inevitable that there's going to be information loss. 
if the term "information loss" is unfamiliar, here's an example that might help. 
lets say that my good friend luigi has been arrested for tax evaision and they're currently being interrogated. behind them is a picture of a clock. 
staring at that image of a clock, can you figure out what time it is? 
It's clear that its' 10 o clock. But is it 10:00 AM or 10:00 PM?
Because this image can "map" to multiple moments in time, it's impossible to know for sure.
In a sense, you can think of the image as being a "lossy" projection of reality.
"Information loss" has occured. 

To get a better sense of "information loss" and how it relates to dimensionality reduction, here's an interactive example. 
we're projecting a set of datapoints onto a line and measuring the "information loss". in this case, it corresponds to the distance between the original points and their projections.

what do you think is the best line to project these points onto?
in this case, it's this line right here (show with a specific u vector set)
when you look at the resulting projection, you can see that the points are still pretty close to their original positions.
the "information loss" is minimal.

compared to projecting the points onto this line (show with a specific u vector set), there is significantly less information loss. 

also looking at the 1d projections, you can see that the points on the good projection are far more "spread out" than the points on the bad projection.

minimization of information loss happens to correspond to maximization of variance. 

to calculate the variance in a given direction, we can project the data onto that direction and measure the spread of the resulting points.

we can also more formally think of this idea as: (show u.T @ X.T @ X @ u)
if you remember from linear algebra, X.T @ X is a symmetric matrix, which means through the spectral theorem, the maximum eigenvalue eigenvectors of the symmetric matrix will end up maximizing the variance. 

here are the top 10 eigenvectors: 
show images. 
it should be interesting to note that you can make out faces. 

despite how horrifying they may be. feel free to take a closer look at and explore these eigenvectors. 

projecting the datapoints onto the top 2 eigenvectors and constructing a new dataset, we can now visualize it in 2d. 




could do some interesting visualizations here: 
represent an image as a 4096 dimensional vector
stack these vectors into an array


